{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Simple\n",
    "\n",
    "## Chapter 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from math import exp\n",
    "\n",
    "def dot_product(xs: List[Float], ys: List[Float]) -> float:\n",
    "    \"\"\"\n",
    "    Dot product of two lists.\n",
    "    \"\"\"\n",
    "    return sum(x * y for x, y in zip(xs, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: float) -> float:\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + exp(-x))\n",
    "\n",
    "def derivative_sigmoid(x: float) -> float:\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron\n",
    "\n",
    "Must have \n",
    "\n",
    "* weights\n",
    "* delta\n",
    "* learning_rate\n",
    "* cache of last output\n",
    "* activation_function\n",
    "* derivative of activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"\n",
    "    Basic unit for a neural network. Must have weights, activation function, derivative of activation function,\n",
    "    learning rate, cache of last output, and delta.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        weights: List[float],\n",
    "        learning_rate: float,\n",
    "        activation_function: Callable[[float], float],\n",
    "        derivative_activation_function: Callable[[float], float],\n",
    "    ) -> None:\n",
    "        self.weights: List[float] = weights\n",
    "        self.activation_function: Callable[[float], float] = activation_function\n",
    "        self.derivative_activation_function: Callable[\n",
    "            [float], float\n",
    "        ] = derivate_activation_function\n",
    "        self.learning_rate: float = learning_rate\n",
    "        self.output_cache: float = 0.0\n",
    "        self.delta: float = 0.0\n",
    "\n",
    "    def output(self, inputs: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        Feed forward pass of neuron.\n",
    "        \"\"\"\n",
    "        self.output_cache = dot_product(inputs, self.weights)\n",
    "        return self.activation_function(self.output_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer\n",
    "\n",
    "A layer must have:\n",
    "\n",
    "* neurons\n",
    "* output cache (after the activation function is applied to neuron's output)\n",
    "* previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Optional\n",
    "from random import random\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Base class for a neural network layer. Must know the previous layer, the neurons, and an output cache (after activation function).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        previous_layer: Optional[Layer],\n",
    "        num_neurons: int,\n",
    "        learning_rate: float,\n",
    "        activation_function: Callable[[float], float],\n",
    "        derivative_activation_function: Callable[[float], float],\n",
    "    ) -> None:\n",
    "        self.previous_layer: Optional[Layer] = previous_layer\n",
    "        self.neurons: List[Neuron] = []\n",
    "\n",
    "        # Add neurons\n",
    "        for i in range(num_neurons):\n",
    "            if previous_layer is None:\n",
    "                random_weights: List[float] = []\n",
    "            else:\n",
    "                # Each neuron is connected to every neuron in previous layer\n",
    "                random_weights = [random() for _ in range(len(previous_layer.neurons))]\n",
    "            neuron: Neuron = Neuron(\n",
    "                random_weights,\n",
    "                learning_rate,\n",
    "                activation_function,\n",
    "                derivative_activation_function,\n",
    "            )\n",
    "            self.neurons.append(neuron)\n",
    "        # Initialize empty output cache\n",
    "        self.output_cache: List[float] = [0.0 for _ in range(num_neurons)]\n",
    "\n",
    "    def outputs(self, inputs: List[float]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Calculate outputs of all neurons in layer.\n",
    "        \"\"\"\n",
    "        if self.previous_layer is None:  # This is an input layer\n",
    "            self.output_cache = inputs\n",
    "        else:\n",
    "            self.output_cache = [n.output(inputs) for n in self.neurons]\n",
    "        return self.output_cache\n",
    "\n",
    "    def calculate_deltas_for_output_layer(self, expected: List[float]) -> None:\n",
    "        \"\"\"\n",
    "        Calculate deltas for output layer. Delta = f'(output_cache) * error\n",
    "        \n",
    "        f'(output_cache) is derivative of activation with respect to output cache\n",
    "        error is expected - actual\n",
    "        \"\"\"\n",
    "        for n in range(len(self.neurons)):\n",
    "            # Call the derivative of the activation function on the neuron's output and\n",
    "            # multiply by the expected value\n",
    "            self.neurons[n].delta = self.neurons[n].derivative_activation_function(\n",
    "                self.neurons[n].output_cache\n",
    "            ) * (expected[n] - self.output_cache[n])\n",
    "\n",
    "    def calculate_deltas_for_hidden_layer(self, next_layer: Layer) -> None:\n",
    "        \"\"\"\n",
    "        Calculate deltas for hiddne layer. Delta = f'(output_cache) * (next_weights * next_deltas)\n",
    "        \"\"\"\n",
    "        for index, neuron in enumerate(self.neurons):\n",
    "            next_weights: List[float] = [n.weights[index] for n in next_layer.neurons]\n",
    "            next_deltas: List[float] = [n.delta for n in next_layer.neurons]\n",
    "            sum_weights_and_deltas: float = dot_product(next_weights, next_deltas)\n",
    "            neuron.delta = (\n",
    "                self.derivative_activation_function(neuron.output_cache)\n",
    "                * sum_weights_and_deltas\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, Tuple\n",
    "from functools import reduce\n",
    "\n",
    "# Output type of interpretation of neural network\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class Network:\n",
    "    \"\"\"\n",
    "    Base class for neural network. Must keep state of layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_structure: List[int],\n",
    "        learning_rate: float,\n",
    "        activation_function: Callable[[float], float] = sigmoid,\n",
    "        derivative_activation_function: Callable[[float], float] = derivative_sigmoid,\n",
    "    ) -> None:\n",
    "        if len(layer_structure) < 3:\n",
    "            raise ValueError(\n",
    "                \"Error: Should be at least 3 layers (input, hidden, output)\"\n",
    "            )\n",
    "\n",
    "        self.layers: List[layer] = []\n",
    "        # Input layer\n",
    "        input_layer: Layer = Layer(\n",
    "            previous_layer=None,\n",
    "            num_neurons=layer_structure[0],\n",
    "            learning_rate=learning_rate,\n",
    "            activation_function=activation_function,\n",
    "            derivative_activation_function=derivative_activation_function,\n",
    "        )\n",
    "        self.layers.append(input_layer)\n",
    "\n",
    "        # Hidden layers and output layer\n",
    "        for previous, num_neurons in enumerate(layer_structure[1::]):\n",
    "            next_layer = Layer(\n",
    "                previous_layer=self.layers[previous],\n",
    "                num_neurons=num_neurons,\n",
    "                learning_rate=learning_rate,\n",
    "                activation_function=activation_function,\n",
    "                derivative_activation_function=derivative_activation_function,\n",
    "            )\n",
    "            self.layers.append(next_layer)\n",
    "\n",
    "    def output(self, input: List[float]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Calculate outputs for entire network.\n",
    "        \"\"\"\n",
    "        # Pushes input data to first layer, then output from first as input to second, output from second\n",
    "        # as input to third and so on\n",
    "        return reduce(\n",
    "            function=lambda inputs, layer: layer.outputs(inputs),\n",
    "            sequence=self.layers,\n",
    "            initial=input,\n",
    "        )\n",
    "\n",
    "    def backpropagate(self, expected: List[float]) -> None:\n",
    "        \"\"\"\n",
    "        Calculate delta (change) for each neuron in each layer. Move backwards through the network\n",
    "        from output towards input.\n",
    "        \"\"\"\n",
    "        # Calculate delta for output\n",
    "        last_layer: int = len(self.layers) - 1\n",
    "        self.layers[last_layer].calculate_deltas_for_output_layer(expected)\n",
    "\n",
    "        # Calculate delta for hidden layers moving from end to beginning\n",
    "        for l in range(last_layer - 1, 0, -1):\n",
    "            # Send in previous layer\n",
    "            self.layers[l].calculate_deltas_for_hidden_layer(self.layers[l + 1])\n",
    "\n",
    "    def update_weights(self) -> None:\n",
    "        \"\"\"\n",
    "        Apply formula to update the weights of all neurons.\n",
    "        \n",
    "        weight = weight + learning_rate * input * neuron_delta\n",
    "        \"\"\"\n",
    "        for layer in self.layers[1:]:\n",
    "            for neuron in layer.neurons:\n",
    "                for w in range(len(neuron.weights)):\n",
    "                    neuron.weights[w] = neuron.weights[w] + (\n",
    "                        neuron.learning_rate\n",
    "                        * layer.previous_layer.output_cache[w]\n",
    "                        * neuron.delta\n",
    "                    )\n",
    "                    \n",
    "    def train(self, inputs: List[List[float]], expecteds: List[List[float]]) -> None:\n",
    "        \"\"\"\n",
    "        Train the network to map from the inputs to the expecteds using the neuron's weights.\n",
    "        \"\"\"\n",
    "        for location, xs in enumerate(inputs):\n",
    "            ys: List[float] = expecteds[location]\n",
    "            outs: List[float] = self.outputs(xs)\n",
    "            # Backpropagate to calculate deltas\n",
    "            self.backpropagate(ys)\n",
    "            # Update weights with deltas\n",
    "            self.update_weights()\n",
    "            \n",
    "    def validate(self, inputs: List[List[float]], expecteds: List[T], interpret_output: Callable[[List[float]], T]) -> Tuple[int, int, float]:\n",
    "        \"\"\"\n",
    "        Validate the network's outputs on a dataset. Returns the correct number of trials and the percentage\n",
    "        correct out of the total. Only applicate for classification tasks. \n",
    "        The callable must interpret the outputs in the problem context.\n",
    "        \"\"\"\n",
    "        correct: int = 0\n",
    "        for input, expected in zip(inputs, expecteds):\n",
    "            # Calculate output from input\n",
    "            outs: List[float] = self.outputs(input)\n",
    "            # Interpret the floats in the data context.\n",
    "            result: T = interpret_output(outs)\n",
    "                \n",
    "            if result == expected:\n",
    "                correct += 1\n",
    "            \n",
    "        percentage: float = correct / len(inputs)\n",
    "        return correct, len(inputs), percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "We will scale inputs to between 0 and 1 for input to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 0.0, 0.0], [0.5, 0.5, 0.5, 0.5]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_by_feature_scaling(dataset: List[[List[float]]]) -> None:\n",
    "    for col_num in range(len(dataset[0])):\n",
    "        column: List[float] = [row[col_num] for row in dataset]\n",
    "        maximum, minimum = max(column), min(column)\n",
    "        for row_num in range(len(column)):\n",
    "            dataset[row_num][col_num] = (dataset[row_num][col_num] - minimum) / (maximum - minimum)\n",
    "            \n",
    "d = [[2, 4, 3, 8], [4, 6, 2, 7], [3, 5, 2.5, 7.5]]\n",
    "normalize_by_feature_scaling(d)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda input, output: input + output, [4, 5, 6], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Classic Iris Dataset\n",
    "\n",
    "This dataset serves as a benchmark for classification models. It is simple, yet will let us ascertain if our model is behaving as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from random import shuffle\n",
    "\n",
    "def read_iris():\n",
    "    \"\"\"\n",
    "    Read in the iris data file as list of lists.\n",
    "    \"\"\"\n",
    "    iris_parameters: List[List[float]] = []\n",
    "    iris_classifications: List[List[float]] = []\n",
    "    iris_species: List[str] = []\n",
    "        \n",
    "    with open('iris.csv', mode='r') as iris_file:\n",
    "        irises: List = list(csv.reader(iris_file))\n",
    "        shuffle(irises) # Mix randomly\n",
    "        for iris in irises:\n",
    "            # The first 4 columns are the features\n",
    "            parameters: List[float] = [float(n) for n in iris[:4]]\n",
    "            iris_parameters.append(parameters)\n",
    "            species: str= iris[4]\n",
    "            if species == 'Iris-setosa':\n",
    "                iris_classifications.append([1.0, 0.0, 0.0])\n",
    "            elif species == 'Iris-versicolor':\n",
    "                iris_classifications.append([0.0, 1.0, 0.0])\n",
    "            elif species == 'Iris-virginica':\n",
    "                iris_classifications.append([0.0, 0.0, 1.0])\n",
    "            iris_species.append(species)\n",
    "            \n",
    "    normalize_by_feature_scaling(iris_parameters)\n",
    "    \n",
    "    return iris_parameters, iris_classifications, iris_species\n",
    "\n",
    "iris_parameters, iris_classifications, iris_species = read_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Iris-versicolor': 50, 'Iris-setosa': 50, 'Iris-virginica': 50})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(iris_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([n[0] for n in iris_parameters])\n",
    "min([n[0] for n in iris_parameters])\n",
    "\n",
    "max([n[1] for n in iris_parameters])\n",
    "min([n[1] for n in iris_parameters])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Interpretation of Model Output for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
