{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Simple\n",
    "\n",
    "## Chapter 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from math import exp\n",
    "\n",
    "def dot_product(xs: List[Float], ys: List[Float]) -> float:\n",
    "    \"\"\"\n",
    "    Dot product of two lists.\n",
    "    \"\"\"\n",
    "    return sum(x * y for x, y in zip(xs, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: float) -> float:\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + exp(-x))\n",
    "\n",
    "def derivative_sigmoid(x: float) -> float:\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron\n",
    "\n",
    "Must have \n",
    "\n",
    "* weights\n",
    "* delta\n",
    "* learning_rate\n",
    "* cache of last output\n",
    "* activation_function\n",
    "* derivative of activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"\n",
    "    Basic unit for a neural network. Must have weights, activation function, derivative of activation function,\n",
    "    learning rate, cache of last output, and delta.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        weights: List[float],\n",
    "        learning_rate: float,\n",
    "        activation_function: Callable[[float], float],\n",
    "        derivative_activation_function: Callable[[float], float],\n",
    "    ) -> None:\n",
    "        self.weights: List[float] = weights\n",
    "        self.activation_function: Callable[[float], float] = activation_function\n",
    "        self.derivative_activation_function: Callable[\n",
    "            [float], float\n",
    "        ] = derivate_activation_function\n",
    "        self.learning_rate: float = learning_rate\n",
    "        self.output_cache: float = 0.0\n",
    "        self.delta: float = 0.0\n",
    "\n",
    "    def output(self, inputs: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        Feed forward pass of neuron.\n",
    "        \"\"\"\n",
    "        self.output_cache = dot_product(inputs, self.weights)\n",
    "        return self.activation_function(self.output_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer\n",
    "\n",
    "A layer must have:\n",
    "\n",
    "* neurons\n",
    "* output cache (after the activation function is applied to neuron's output)\n",
    "* previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Optional\n",
    "from random import random\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Base class for a neural network layer. Must know the previous layer, the neurons, and an output cache (after activation function).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        previous_layer: Optional[Layer],\n",
    "        num_neurons: int,\n",
    "        learning_rate: float,\n",
    "        activation_function: Callable[[float], float],\n",
    "        derivative_activation_function: Callable[[float], float],\n",
    "    ) -> None:\n",
    "        self.previous_layer: Optional[Layer] = previous_layer\n",
    "        self.neurons: List[Neuron] = []\n",
    "\n",
    "        # Add neurons\n",
    "        for i in range(num_neurons):\n",
    "            if previous_layer is None:\n",
    "                random_weights: List[float] = []\n",
    "            else:\n",
    "                # Each neuron is connected to every neuron in previous layer\n",
    "                random_weights = [random() for _ in range(len(previous_layer.neurons))]\n",
    "            neuron: Neuron = Neuron(\n",
    "                random_weights,\n",
    "                learning_rate,\n",
    "                activation_function,\n",
    "                derivative_activation_function,\n",
    "            )\n",
    "            self.neurons.append(neuron)\n",
    "        # Initialize empty output cache\n",
    "        self.output_cache: List[float] = [0.0 for _ in range(num_neurons)]\n",
    "\n",
    "    def outputs(self, inputs: List[float]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Calculate outputs of all neurons in layer.\n",
    "        \"\"\"\n",
    "        if self.previous_layer is None:  # This is an input layer\n",
    "            self.output_cache = inputs\n",
    "        else:\n",
    "            self.output_cache = [n.output(inputs) for n in self.neurons]\n",
    "        return self.output_cache\n",
    "\n",
    "    def calculate_deltas_for_output_layer(self, expected: List[float]) -> None:\n",
    "        \"\"\"\n",
    "        Calculate deltas for output layer. Delta = f'(output_cache) * error\n",
    "        \n",
    "        f'(output_cache) is derivative of activation with respect to output cache\n",
    "        error is expected - actual\n",
    "        \"\"\"\n",
    "        for n in range(len(self.neurons)):\n",
    "            # Call the derivative of the activation function on the neuron's output and\n",
    "            # multiply by the expected value\n",
    "            self.neurons[n].delta = self.neurons[n].derivative_activation_function(\n",
    "                self.neurons[n].output_cache\n",
    "            ) * (expected[n] - self.output_cache[n])\n",
    "            \n",
    "    def calculate_deltas_for_hidden_layer(self, next_layer: Layer) -> None:\n",
    "        \"\"\"\n",
    "        Calculate deltas for hiddne layer. Delta = f'(output_cache) * (next_weights * next_deltas)\n",
    "        \"\"\"\n",
    "        for index, neuron in enumerate(self.neurons):\n",
    "            next_weights: List[float] = [n.weights[index] for n in next_layer.neurons]\n",
    "            next_deltas: List[float] = [n.delta for n in next_layer.neurons]\n",
    "            sum_weights_and_deltas: float = dot_product(next_weights, next_deltas)\n",
    "            neuron.delta = self.derivative_activation_function(neuron.output_cache) * sum_weights_and_deltas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
